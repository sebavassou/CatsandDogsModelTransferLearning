{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/0+btCAtjjQM/aZk6OR59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebavassou/CatsandDogsModelTransferLearning/blob/main/CatsandDogsModelTransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visão Geral do Projeto\n",
        "O objetivo é aplicar Transfer Learning em uma rede neural profunda usando Python no ambiente Google Colab. O projeto se baseará no exemplo do MNIST, mas será adaptado para classificar imagens de gatos e cachorros.\n",
        "Dataset Cats vs Dogs\n",
        "O dataset \"Cats vs Dogs\" é uma coleção extensa de imagens de gatos e cachorros, ideal para tarefas de classificação binária2. Algumas características importantes do dataset incluem:\n",
        "•\tTamanho total: 1.04 GiB\n",
        "•\tNúmero de exemplos: 23,262 imagens no conjunto de treinamento\n",
        "•\tFormato das imagens: RGB (3 canais de cor)\n",
        "•\tClasses: 2 (gato e cachorro)\n",
        "É importante notar que o dataset original continha algumas imagens corrompidas, mas essas foram removidas na versão atual2.\n",
        "Implementação do Transfer Learning\n",
        "Para implementar o Transfer Learning, seguimos estas ações:\n",
        "1.\tCarregar o dataset: Use a API do TensorFlow Datasets para carregar os dados:\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset, info = tfds.load('cats_vs_dogs', with_info=True, as_supervised=True)\n",
        "\n",
        "2.\tPré-processar as imagens: Redimensione e normalize as imagens para se adequarem ao modelo pré-treinado que você escolher.\n",
        "\n",
        "3.\tEscolher um modelo base: Selecione um modelo pré-treinado, como VGG16, ResNet50 ou InceptionV3, disponível no Keras.\n",
        "\n",
        "4.\tCongelar as camadas base: Para preservar os pesos pré-treinados:\n",
        "python\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "5.\tAdicionar novas camadas: Adicione camadas densas no topo do modelo base para a classificação específica de gatos e cachorros.\n",
        "\n",
        "6.\tCompilar e treinar: Compile o modelo com um otimizador adequado e treine-o com os dados de gatos e cachorros.\n",
        "\n",
        "7.\tFine-tuning: Opcionalmente, descongele algumas camadas superiores do modelo base e faça um fine-tuning com uma taxa de aprendizado baixa.\n",
        "\n",
        "Considerações Adicionais:\n",
        "\n",
        "•\tO dataset está disponível para download direto da Microsoft, com um tamanho de 786.7 MB\n",
        "•\tUtilizamos o Google Colab para aproveitar os recursos de GPU gratuitos, o que pode acelerar significativamente o treinamento do modelo.\n",
        "•\tConsideramos usar técnicas de aumento de dados (data augmentation) para melhorar a generalização do modelo, especialmente considerando o número relativamente pequeno de imagens no dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rtdk7k_-PKHB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVBk-HNHYnUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "otz6wlZCPF7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas necessárias\n",
        "#import tensorflow as tf\n",
        "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from tensorflow.keras.applications import VGG16\n",
        "#from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "#from tensorflow.keras.models import Model\n",
        "#import matplotlib.pyplot as plt\n",
        "#import os\n",
        "#from PIL import UnidentifiedImageError"
      ],
      "metadata": {
        "id": "Fe7XrPyyY0ec"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow_heif\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import pillow_heif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh6P6kGDGT70",
        "outputId": "d19c7ab1-465c-4647-802f-b9101909aee9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow_heif\n",
            "  Downloading pillow_heif-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow_heif) (11.1.0)\n",
            "Downloading pillow_heif-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/7.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m5.5/7.8 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow_heif\n",
            "Successfully installed pillow_heif-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando o TensorFlow para usar GPU, se disponível\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "3EuHewVtY-Is"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo parâmetros\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "rD-guuQ0Y_Fr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixando e extraindo o dataset\n",
        "!wget https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zipAa\n",
        "!unzip -q kagglecatsanddogs_5340.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAh3ay3wY_Mk",
        "outputId": "f081739b-e489-45ac-fb58-3d35ba974a1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-18 17:28:45--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zipAa\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 184.29.22.44, 2600:1406:bc00:a81::317f, 2600:1406:bc00:a8e::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|184.29.22.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-02-18 17:28:46 ERROR 404: Not Found.\n",
            "\n",
            "replace PetImages/Cat/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace PetImages/Cat/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparando os geradores de dados\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'PetImages',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    'PetImages',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny_vh1QrY_Q0",
        "outputId": "e13889e2-37af-4c9b-8c13-19494c5c1b99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 images belonging to 2 classes.\n",
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o modelo base VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n"
      ],
      "metadata": {
        "id": "9vApBkvGY_U-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionando novas camadas\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)"
      ],
      "metadata": {
        "id": "Ri_uYTMxY_Yb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilando o modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uK8hh1nAY_b0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle image loading errors\n",
        "def load_image_with_error_handling(img_path):\n",
        "    try:\n",
        "        # Attempt to open the image using Pillow\n",
        "        img = Image.open(img_path)\n",
        "        # Convert HEIC images to RGB if needed\n",
        "        if img.format == \"HEIF\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        # Resize and convert to array\n",
        "        img = img.resize(IMG_SIZE)\n",
        "        img = tf.keras.utils.img_to_array(img)\n",
        "        return img\n",
        "    except (UnidentifiedImageError, OSError) as e:\n",
        "        print(f\"Error loading image {img_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "A6m3EsDtZjsC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom generator with error handling\n",
        "def data_generator(generator):\n",
        "    while True:\n",
        "        batch_x, batch_y = next(generator)\n",
        "        # Filter out images that couldn't be loaded\n",
        "        valid_indices = [i for i, img in enumerate(batch_x) if img is not None]\n",
        "        if valid_indices:\n",
        "            yield batch_x[valid_indices], batch_y[valid_indices]\n",
        "        else:\n",
        "            # If all images in the batch are invalid, skip the batch\n",
        "            continue"
      ],
      "metadata": {
        "id": "vJvSFoy3Zjww"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom generators for training and validation\n",
        "train_generator_wrapped = data_generator(train_generator)\n",
        "validation_generator_wrapped = data_generator(validation_generator)\n"
      ],
      "metadata": {
        "id": "xRnn4GT-Zj5H"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "# Now use the wrapped generators\n",
        "history = model.fit(\n",
        "    train_generator_wrapped,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=validation_generator_wrapped,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IWtjGIv9ZkCo",
        "outputId": "dd213644-4a1d-4e67-b567-eedfcb2cf804"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m173/625\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:13\u001b[0m 428ms/step - accuracy: 0.7403 - loss: 0.5259"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m293/625\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 428ms/step - accuracy: 0.7738 - loss: 0.4752"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/generator_data_adapter.py\", line 52, in get_tf_iterator\n    for batch in self.generator():\n\n  File \"<ipython-input-24-e99e7bdded74>\", line 4, in data_generator\n    batch_x, batch_y = next(generator)\n                       ^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 112, in __next__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3532, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/generator_data_adapter.py\", line 52, in get_tf_iterator\n    for batch in self.generator():\n\n  File \"<ipython-input-24-e99e7bdded74>\", line 4, in data_generator\n    batch_x, batch_y = next(generator)\n                       ^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 112, in __next__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3532, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_16917]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ceecc7681626>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Treinando o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Now use the wrapped generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_generator_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/generator_data_adapter.py\", line 52, in get_tf_iterator\n    for batch in self.generator():\n\n  File \"<ipython-input-24-e99e7bdded74>\", line 4, in data_generator\n    batch_x, batch_y = next(generator)\n                       ^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 112, in __next__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3532, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) UNKNOWN:  UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/generator_data_adapter.py\", line 52, in get_tf_iterator\n    for batch in self.generator():\n\n  File \"<ipython-input-24-e99e7bdded74>\", line 4, in data_generator\n    batch_x, batch_y = next(generator)\n                       ^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 112, in __next__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\", line 313, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_utils.py\", line 236, in load_img\n    img = pil_image.open(io.BytesIO(f.read()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3532, in open\n    raise UnidentifiedImageError(msg)\n\nPIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7e4b4fd664d0>\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_16917]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotando os resultados\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Treino')\n",
        "plt.plot(history.history['val_accuracy'], label='Validação')\n",
        "plt.title('Acurácia do Modelo')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Treino')\n",
        "plt.plot(history.history['val_loss'], label='Validação')\n",
        "plt.title('Perda do Modelo')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Perda')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "9j9pF-qdY_gW",
        "outputId": "93bf085f-f78f-42e7-a1e0-953cb360915c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bfb87e0b9f6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Treino'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validação'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Acurácia do Modelo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning (opcional)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "93Pob_ZrY_j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvando o modelo\n",
        "model.save('cats_vs_dogs_model.h5')\n"
      ],
      "metadata": {
        "id": "8V7wXe1zY_n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pe09KPNrY_8z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}